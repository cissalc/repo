1 解压
  tar -zxf  spark-1.3.1-bin-hadoop2.6.tgz -C /home/hadoop
  mv spark-1.3.1-bin-hadoop2.6 spark-1.3.1

2 配置环境变量
	sudo vi /etc/profile
		export SPARK_HOME=/home/hadoop/spark-1.3.1
		export PATH=$PATH:$SPARK_HOME/bin
	source /etc/profile
	
3 配置spark-1.3.1 
	1) 进入spark的conf目录
		cp  spark-env.sh.template spark-env.sh
		vi  spark-env.sh
			export JAVA_HOME=/opt/jdk1.7.0_71
			export SCALA_HOME=/home/hadoop/scala-2.11.6
			export SPARK_MASTER_IP=master
			export SPARK_WORKER_MEMORY=1g
			export HADOOP_CONF_DIR=/home/hadoop/hadoop-2.6.0/etc/hadoop
			export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native"
	2)cp slaves.template slaves
	  master
	  slave01
	  slave02
	  
4 把spark复制到其他节点
	scp -r spark-1.3.1 hadoop@slave01:/home/hadoop/
	scp -r spark-1.3.1 hadoop@slave02:/home/hadoop/
	
5 修改虚拟阶环境变量同3

6 启动集群
	1）启动zookeeper, 每台运行
	   zkServer.sh start
	2）启动hadoop集群,sbin下
		 ./start-all.sh
	3）启动spark集群
		/home/hadoop/spark-1.3.1/sbin/start-all.sh
	4）浏览器访问：http://master:8080/